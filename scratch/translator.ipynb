{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bf47f195-66df-4daa-8e71-43aadc5c15a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8515a737-11c3-44b9-b8d7-57882b1e4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.trainers import BpeTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6240bdb2-2207-4603-b5bd-9d1afcc27d1b",
   "metadata": {},
   "source": [
    "## get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29cc56a2-e384-4b45-88f8-0d13f23d9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_train_path = \"/home/adam/play/translate_data/train.de\"\n",
    "eng_train_path = \"/home/adam/play/translate_data/train.en\"\n",
    "\n",
    "ger_train = open(ger_train_path, \"r\").readlines()\n",
    "eng_train = open(eng_train_path, \"r\").readlines()\n",
    "\n",
    "# just to help my poor laptop\n",
    "ger_train = ger_train[:10000]\n",
    "eng_train = eng_train[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b342e7e-bd9c-489d-9cf0-f675475d0cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(ger_train))\n",
    "print(len(eng_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e981672b-1d7a-4ca9-907a-55240ad8540f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### tokenize \n",
    "tokenizer = Tokenizer(BPE())\n",
    "\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]) # do i need to account for the weird #AT things??\n",
    "tokenizer.train(files=[ger_train_path, eng_train_path,], trainer=trainer)\n",
    "tokenizer.enable_padding()\n",
    "# tokenizer.train_from_iterator(iter(ger_train + eng_train), trainer=trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55d01b5a-9e04-4196-a256-71e7bd72ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_train = tokenizer.encode_batch(ger_train)\n",
    "eng_train = tokenizer.encode_batch(eng_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6ce4708-7281-4a8f-9cad-41582ff471e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ger_train = torch.tensor([t.ids for t in ger_train])\n",
    "eng_train = torch.tensor([t.ids for t in eng_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76ac0bce-b8da-4852-b2b4-929d4c4bfad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 258])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ger_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781269b0-71c1-4034-8dc1-db1fd038c0b7",
   "metadata": {},
   "source": [
    "## define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24367e26-1410-4010-96e5-4ed5610d9b21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPLAN\\n\\n1. Create multi-headed attention block\\n2. Create encoder blocks (should just be MHA + layer-norm + FF) \\n3. create decoder blocks (needs both normal attention + weird feature attention)\\n4. create embedding \\n5. create positional embedding (if absolute pos is possible I will use, otherwise I will be forced to enage with horrible sinusoidals etc)\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "PLAN\n",
    "\n",
    "1. Create attention + multi-headed attention block\n",
    "2. Create encoder blocks (should just be MHA + layer-norm + FF) \n",
    "3. create decoder blocks (needs both normal attention + weird feature attention)\n",
    "4. create embedding \n",
    "5. create positional embedding (if absolute pos is possible I will use, otherwise I will be forced to enage with horrible sinusoidals etc)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "411dbe29-89cd-415a-8abc-a96b4322b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, n_embed, head_size, dropout, needs_mask=True, store_kv=False):\n",
    "        super().__init__()\n",
    "        self.head_size = head_size\n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False) # (n_embed, head_size)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.needs_mask = needs_mask\n",
    "        self.store_kv = store_kv\n",
    "        if self.needs_mask:\n",
    "            self.register_buffer(\"tril\", torch.tril(torch.ones(head_size, head_size)))\n",
    "    \n",
    "    def forward(self, k_in, q_in, v_in):\n",
    "        B, T, C = k_in.shape # (batch, context_size, n_embed)\n",
    "        # create k and q matrices, use them to create attention matrix\n",
    "    \n",
    "        k = self.key(k_in) # (B, context_size, n_embed) @ (n_embed, head_size) --> (B, context_size, head_size)\n",
    "        q = self.query(q_in) # (B, context_size, n_embed) @ (n_embed, head_size) --> (B, context_size, head_size)\n",
    "        v = self.value(v_in) # (B, context_size, n_embed) @ (n_embed, head_size) --> (B, context_size, head_size)\n",
    "\n",
    "        # if we're in the Encoder block then store the k and v\n",
    "        if self.store_kv:           \n",
    "            self.k = k\n",
    "            self.v = v\n",
    "\n",
    "        attn = q @ k.transpose(-2,-1) * self.head_size**-0.5  # (B, context_size, head_size) @ (B, head_size, context_size) --> (B, context_size, context_size)\n",
    "        \n",
    "        # if this is a masked attention layer (causal?) mask out all tokens before the cur pos \n",
    "        if self.needs_mask:\n",
    "            attn = attn.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) # NOTE: still not sure why tril needs to be index up to T, shouldn't that be the same as its size??\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        # generate the v matrix and use the attn matrix to pluck out relevant info\n",
    "        out = attn @ v # (B, context_size, context_size) @ (B, context_size, head_size) --> (B, context_size, head_size)\n",
    "        return out\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_embed, n_heads, dropout, needs_mask=True, store_kv=False):\n",
    "        super().__init__()\n",
    "        self.n_embed = n_embed\n",
    "        self.n_heads = n_heads\n",
    "        assert(n_embed % n_heads == 0) # check dims work\n",
    "        self.n_head_size = n_embed / n_heads\n",
    "        self.dropout = dropout\n",
    "        self.needs_mask = needs_mask\n",
    "        self.store_kv = store_kv\n",
    "        if self.needs_mask:\n",
    "            self.register_buffer(\"tril\", torch.tril(torch.ones(head_size, head_size)))\n",
    "\n",
    "        self.wk = self.Linear(n_embed, n_embed, bias=False)\n",
    "        self.wq = self.Linear(n_embed, n_embed, bias=False)\n",
    "        self.wv = self.Linear(n_embed, n_embed, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, k_in, q_in, v_in):\n",
    "        B, T, C = k_in.shape # (batch, context_size, n_embed)\n",
    "\n",
    "        # create k\n",
    "        # NOTE: make sure the maths works here\n",
    "        k = self.wk(k_in) # (B, context_size, n_embed) @ (n_embed, n_embed) ---> (B, context_size, n_embed)\n",
    "        # split per head\n",
    "        k = k.view(B, T, self.n_heads, self.head_size)  # (B, context_size, n_embed) --> (B, context_size, n_heads, head_size)\n",
    "        # switch context size and n_heads dim so we can batch matmul over B and n_heads\n",
    "        k = k.transpose(1,2) # (B, context_size, n_heads, head_size) --> (B, n_heads, context_size, head_size)\n",
    "        \n",
    "        # create q \n",
    "        q = self.wq(q_in)\n",
    "        q = q.view(B, T, self.n_heads, self.head_size)\n",
    "        q = q.transpose(1,2)\n",
    "\n",
    "        # create v \n",
    "        v = self.wv(v_in)\n",
    "        v = v.view(B, T, self.n_heads, self.head_size)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        attn = q @ k.transpose(-2, -1) # (B, n_heads, context_size, head_size) @ (B, n_heads, head_size, context_size) --> (B, n_heads, context_size, context_size)\n",
    "        \n",
    "        # if this is a masked attention layer (causal?) mask out all tokens before the cur pos \n",
    "        if self.needs_mask:\n",
    "            attn = attn.masked_fill(self.tril[:T,:T] == 0, float(\"-inf\")) # NOTE: still not sure why tril needs to be index up to T, shouldn't that be the same as its size??\n",
    "\n",
    "        attn = self.dropout(F.softmax(attn, dim=-1))\n",
    "        \n",
    "        # generate the v matrix and use the attn matrix to pluck out relevant info on a per head basis\n",
    "        out = attn @ v # (B, n_heads, context_size, context_size) @ (B, n_heads, context_size, head_size) --> (B, n_heads, context_size, head_size)\n",
    "        # remove per-head dimension and use final linear projection\n",
    "        out = out.view(B, T, self.n_embed) # (B, n_heads, context_size, head_size) --> (B, context_size, n_embed)\n",
    "        return self.proj(out)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, dropout, needs_mask=True, store_kv=False):\n",
    "        super().__init__()\n",
    "        head_size = n_embed // n_heads\n",
    "        self.heads = nn.ModuleList([AttentionHead(n_embed, head_size, dropout, needs_mask, store_kv) for _ in range(n_heads)])\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    def forward(self, k_in, q_in, v_in):\n",
    "        x = torch.cat([head(k_in, q_in, v_in) for head in self.heads], dim=-1) # concat( (B, context_size, head_size) ) --> (B, context_size, n_embed)\n",
    "        return self.proj(x) # (B, context_size, n_embed) @ (n_embed, n_embed) --> (B, context_size, n_embed)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.ffw = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4* n_embed),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embed, n_embed),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ffw(x) # (B, context_size, n_embed) @ (n_embed, 4*n_embed) @ (4*n_embed, n_embed) --> (B, context_size, n_embed)\n",
    "\n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(n_heads, n_embed, dropout, False, True)\n",
    "        self.ffw = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.attention(x, x, x)\n",
    "        out = self.ln1(out)\n",
    "        out = self.ffw(out)\n",
    "        out = self.ln2(out)\n",
    "        return x + out # NOTE: not clear if this is the right place for adding to take place - refer to other implementations\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, dropout):\n",
    "        super().__init__()\n",
    "        self.masked_attention = MultiHeadAttention(n_heads, n_embed, dropout, True)\n",
    "        self.attention = MultiHeadAttention(n_heads, n_embed, dropout, True)\n",
    "        self.ffw = FeedForward(n_embed, dropout)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "        self.ln3 = nn.LayerNorm(n_embed)\n",
    "\n",
    "    def forward(self, x, k_in, v_in):\n",
    "        x = self.masked_attention(x, x, x)\n",
    "        x = self.ln1(x)\n",
    "        x = self.attention(k_in, x, v_in)\n",
    "        x = self.ln2(x)\n",
    "        x = self.ffw(x)\n",
    "        x = self.ln3(x)\n",
    "        return x\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_embed, context_size):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        posem = torch.zeros((context_size, n_embed))\n",
    "        for pos in range(context_size):\n",
    "            for i in range(n_embed, 2):\n",
    "                posem[pos, i] = math.sin(pos/(10000**(2*i/n_embed)))\n",
    "                posem[pos, i+1] = math.cos(pos/(10000**(2*(i+1)/n_embed)))\n",
    "        posem.unsqueeze(0)\n",
    "        self.register_buffer(\"posem\", posem)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.posem[torch.arange(x.shape[1]),:]\n",
    "\n",
    "\n",
    "class EncoderDecoderTransformer(nn.Module):\n",
    "    def __init__(self, n_heads, n_embed, dropout, n_blocks, context_size, input_vocab_size, output_vocab_size):\n",
    "        super().__init__()\n",
    "        self.input_embedding = nn.Embedding(input_vocab_size, n_embed)\n",
    "        self.positional_embedding = PositionalEmbedding(n_embed, context_size)\n",
    "        self.encoders = nn.Sequential(*[EncoderBlock(n_heads, n_embed, dropout) for _ in range(n_blocks)])\n",
    "        self.decoders = nn.Sequential(*[DecoderBlock(n_heads, n_embed, dropout) for _ in range(n_blocks)])\n",
    "        self.output_embedding = nn.Embedding(output_vocab_size, n_embed) # NOTE: should this really be the same vocab size as input?? \n",
    "\n",
    "    def forward(self, xi, xo):\n",
    "        x = self.input_embedding(xi)\n",
    "        x = self.positional_embedding(x)\n",
    "        print(f\"post emb {x.shape}\")\n",
    "        x = self.encoders(x)\n",
    "        print(f\"post enc {x.shape}\")\n",
    "        x = self.decoders(xo , self.encoders[-1].k, self.encoders[-1].v)\n",
    "        print(f\"post dec {x.shape}\")\n",
    "        x = self.output_embedding(x)\n",
    "        return F.softmax(x, dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e409cb06-f994-47c8-8679-54fd7938f769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "post emb torch.Size([2, 12, 60])\n",
      "post enc torch.Size([2, 12, 60])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'EncoderBlock' object has no attribute 'k'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m src \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     10\u001b[0m                     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[1;32m     11\u001b[0m target \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m], \n\u001b[1;32m     12\u001b[0m                        [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m---> 13\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/translate-gpt/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/translate-gpt/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[105], line 127\u001b[0m, in \u001b[0;36mEncoderDecoderTransformer.forward\u001b[0;34m(self, xi, xo)\u001b[0m\n\u001b[1;32m    125\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders(x)\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost enc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 127\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoders(xo , \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoders\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoders[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mv)\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost dec \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_embedding(x)\n",
      "File \u001b[0;32m~/dev/translate-gpt/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'EncoderBlock' object has no attribute 'k'"
     ]
    }
   ],
   "source": [
    "# TODO: work out how to handle extracting k and v from multiple heads in final encoder block?? very possibly I should just rewrite in the kombo-head pattern \n",
    "\n",
    "model = EncoderDecoderTransformer(\n",
    "    n_heads=3, n_embed=60, dropout=0.2, n_blocks=5, context_size=12, input_vocab_size=11, output_vocab_size=11\n",
    ")\n",
    "\n",
    "# x = torch.arange(11)\n",
    "# y = torch.tensor([[0]])\n",
    "src = torch.tensor([[0, 2, 5, 6, 4, 3, 9, 5, 2, 9, 10, 1], \n",
    "                    [0, 2, 8, 7, 3, 4, 5, 6, 7, 2, 10, 1]])\n",
    "target = torch.tensor([[0, 1, 7, 4, 3, 5, 9, 2, 8, 10, 9, 1], \n",
    "                       [0, 1, 5, 6, 2, 4, 7, 6, 2, 8, 10, 1]])\n",
    "model(src, target[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9df405d9-16af-4a4f-be26-48f76da33172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ae372c0-8e11-4f52-8cf9-500f5026f07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 12])\n"
     ]
    }
   ],
   "source": [
    "print(src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af586f39-10f4-4497-86b7-806af5c1eae4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7dd8ed-52c5-46c1-b041-5d5aaa43d581",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dfda2e7e-4297-4106-9a72-6c729c9de94e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "bc48ce6f-217b-49ef-8c4f-be8bdf721c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.ones((5,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "cb856dbf-e1a1-4cd2-ba02-43ea272ad4db",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[5, 3, 2]' is invalid for input of size 15",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[111], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[5, 3, 2]' is invalid for input of size 15"
     ]
    }
   ],
   "source": [
    "a.view(5,3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "08a3c758-d040-4110-b22e-695f470b5c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = nn.Linear(5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "35190d8a-4cd7-4b36-842a-07872b9e10d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Linear' object has no attribute 'view'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[114], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n",
      "File \u001b[0;32m~/dev/translate-gpt/.env/lib/python3.11/site-packages/torch/nn/modules/module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Linear' object has no attribute 'view'"
     ]
    }
   ],
   "source": [
    "b.view(3, 5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83629a7f-0617-4bf6-aa7f-5924a1846552",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
